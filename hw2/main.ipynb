{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дз 2\n",
    "1. Я выбрал тексты из постов группы вк [Карусель Заря](https://vk.com/club168307086). Думаю они подходят так как там довольно разнообразный лексикон и встречаются необычные словоформы.\n",
    "2. Три теггера - stanza, pymorphy, natasha. Часть сравнения теггеров находится в папке [tournament](./tournament/). \n",
    "    - [main.py](./tournament/main.py) прогоняет весь турнир и выводит результаты accuracy. \n",
    "    - [tag_manual.py](./tournament/tag_manual.py) простой скриптик с UI для разметки референсного корпуса.\n",
    "    - [tag_natasha.py](./tournament/tag_natasha.py), [tag_pymorphy.py](./tournament/tag_pymorphy.py), [tag_stanza.py](./tournament/tag_stanza.py) - модули, размечающие текст используя соответсвующий тэггер и сохранающие результаты в файлы в [parsed_data](./tournament/parsed_data/). \n",
    "    - [tag_toolkit.py](./tournament/tag_toolkit.py) - функции помошники для читабельности и компактности кода\n",
    "3. accuracy выводится в конце исполнения [main.py](./tournament/main.py). Конвертер POS тэгов пришлось написать для pymorphy тк он использует не universal pos tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kesha/dev/hse/nlp_hw/hw2/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:root:Ready data paths:\n",
      "{'natasha': '/home/kesha/dev/hse/nlp_hw/hw2/tournament/parsed_data/natasha.xml',\n",
      " 'pymorphy': '/home/kesha/dev/hse/nlp_hw/hw2/tournament/parsed_data/pymorphy.xml',\n",
      " 'stanza': '/home/kesha/dev/hse/nlp_hw/hw2/tournament/parsed_data/stanza.xml'}\n",
      "INFO:tournament.main:Accuracy of NATASHA: 0.82464\n",
      "INFO:tournament.main:Accuracy of PYMORPHY: 0.80569\n",
      "INFO:tournament.main:Accuracy of STANZA: 0.83886\n"
     ]
    }
   ],
   "source": [
    "from tournament import run_tournament\n",
    "run_tournament()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Входные данные - 1000 положительных и 1000 отризательных отзывов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "bigrams_path = Path('./ngrams/data/bigrams.json')\n",
    "trigrams_path = Path('./ngrams/data/trigrams.json')\n",
    "do_parse_ngrams = not (bigrams_path.exists() and trigrams_path.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные и парсим их, считаем частотность всех лемматизированых биграм и триграм с помощью своего модуля [parse](./ngrams/_parse.py). (Только если частотности n-грамм ещё не посчитаны)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 19:01:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 6.75MB/s]                    \n",
      "2023-10-07 19:01:40 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "INFO:stanza:Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2023-10-07 19:01:40 INFO: Using device: cpu\n",
      "INFO:stanza:Using device: cpu\n",
      "2023-10-07 19:01:40 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2023-10-07 19:01:41 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2023-10-07 19:01:42 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2023-10-07 19:01:42 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "100%|██████████| 1000/1000 [14:57<00:00,  1.11it/s]\n",
      "100%|██████████| 1000/1000 [17:53<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from json import load, dump\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "if do_parse_ngrams:\n",
    "    from ngrams import parse_review, data_file\n",
    "\n",
    "    pos_bigrams = Counter(); pos_trigrams = Counter()\n",
    "    neg_bigrams = Counter(); neg_trigrams = Counter()\n",
    "\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        data = load(f)\n",
    "\n",
    "    for review in tqdm(data['pos'][:1000]):\n",
    "        bigrams, trigrams = parse_review(review)\n",
    "        pos_bigrams.update(bigrams)\n",
    "        pos_trigrams.update(trigrams)\n",
    "    for review in tqdm(data['neg'][:1000]):\n",
    "        bigrams, trigrams = parse_review(review)\n",
    "        neg_bigrams.update(bigrams)\n",
    "        neg_trigrams.update(trigrams)\n",
    "\n",
    "    f\"pos: {(len(pos_bigrams), len(pos_trigrams))}, neg: {(len(neg_bigrams), len(neg_trigrams))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормируем частотности и фильтруем малчастотные n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(count: Counter) -> dict:\n",
    "    minimun_count = 5\n",
    "    total = count.total()\n",
    "    return { k: v/total for k, v in count.items() if v >= minimun_count }\n",
    "if do_parse_ngrams:\n",
    "    pos_bigrams = normalize(pos_bigrams)\n",
    "    pos_trigrams = normalize(pos_trigrams)\n",
    "    neg_bigrams = normalize(neg_bigrams)\n",
    "    neg_trigrams = normalize(neg_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совмещаем биграммы с биграммами, триграммы с триграммами, а разделение на негативные и положительные указываем во внутреннем словаре.\n",
    "\n",
    "Т.е. \n",
    "```\n",
    "# Положительные отзывы\n",
    "{\n",
    "    'хороший + NOUN': 12,\n",
    "    'не + VERB': 6\n",
    "}\n",
    "# Отрицательные отзывы\n",
    "{\n",
    "    'ужасно + VERB': 32,\n",
    "    'не + VERB': 16\n",
    "}\n",
    "```\n",
    "превратится в\n",
    "```\n",
    "{\n",
    "    'хороший + NOUN': {\n",
    "        'pos': 12,\n",
    "        'neg': 0\n",
    "    },\n",
    "    'ужасно + VERB': {\n",
    "        'pos': 0,\n",
    "        'neg': 32\n",
    "    },\n",
    "    'не + VERB': {\n",
    "        'pos': 6,\n",
    "        'neg': 16\n",
    "    },\n",
    "}\n",
    "```\n",
    "Экономия памяти и удобство для последующего сравнения в наших целях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6389 bigrams, 10601 trigrams\n"
     ]
    }
   ],
   "source": [
    "def merge_ngrams(pos_ngrams, neg_ngrams) -> dict:\n",
    "    all_ngrams = set(pos_ngrams.keys())\n",
    "    all_ngrams.update(neg_ngrams.keys())\n",
    "    result = {}\n",
    "    for ngram in all_ngrams:\n",
    "        result[ngram] = {\n",
    "            'pos': pos_ngrams[ngram] if ngram in pos_ngrams else 0,\n",
    "            'neg': neg_ngrams[ngram] if ngram in neg_ngrams else 0,\n",
    "        }\n",
    "    return result\n",
    "if do_parse_ngrams:\n",
    "    bigrams = merge_ngrams(pos_bigrams, neg_bigrams)\n",
    "    del pos_bigrams, neg_bigrams\n",
    "    trigrams = merge_ngrams(pos_trigrams, neg_trigrams)\n",
    "    del pos_trigrams, neg_trigrams\n",
    "    with open(bigrams_path, 'w') as f:\n",
    "        dump(bigrams, f, ensure_ascii=False, indent=4)\n",
    "    with open(trigrams_path, 'w') as f:\n",
    "        dump(trigrams, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"{len(bigrams)} bigrams, {len(trigrams)} trigrams\")\n",
    "else:\n",
    "    # Если файлы уже есть то просто их загружаем:\n",
    "    with open(bigrams_path, 'r') as f:\n",
    "        bigrams = load(f)\n",
    "    with open(trigrams_path, 'r') as f:\n",
    "        trigrams = load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее выявляем n-граммы, обладающие наибольшей разностью частотности между позитивными и негативными отзывами.\n",
    "\n",
    "Задать критерий сортировки можно двумя способами: \n",
    "- разность между частотностями `get_top_diff()`. n-грамма с P('pos')=0.05,P('neg')=0.07 будет считаться БОЛЕЕ негативной чем n-грамма с P('pos')=0.01,P('neg')=0.02 т.е. в этом случае 2-1 < 7-5\n",
    "- соотношениие частотностей `get_top_scale()`. n-грамма с P('pos')=0.05,P('neg')=0.07 будет считаться МЕНЕЕ негативной чем n-грамма с P('pos')=0.01,P('neg')=0.02 т.е. в этом случае 2/1 > 7/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_diff(ngrams, target, other, n=5):\n",
    "    def comparison_key(x):\n",
    "        return x[1][target] - x[1][other]\n",
    "    sorted_top = sorted(ngrams.items(), key=comparison_key, reverse=True)[:n]\n",
    "    return sorted_top\n",
    "def get_top_scale(ngrams, target, other, n=5):\n",
    "    def comparison_key(x):\n",
    "        if x[1][other] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return x[1][target] / x[1][other]\n",
    "    sorted_top = sorted(ngrams.items(), key=comparison_key, reverse=True)[:n]\n",
    "    return sorted_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но `get_top_diff()` хоть и кажется логичнее и первым пришло в голову, но в данной ситуации оно выдаёт не такой интересный результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 only positive bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('T_ADJ + T_NOUN',\n",
       "  {'pos': 0.016079781704781703, 'neg': 0.011468401486988848}),\n",
       " ('T_NOUN + T_NOUN',\n",
       "  {'pos': 0.013831860706860707, 'neg': 0.011891263940520445}),\n",
       " ('T_ADV + T_ADJ',\n",
       "  {'pos': 0.003241943866943867, 'neg': 0.0015938661710037175}),\n",
       " ('T_NOUN + T_ADJ',\n",
       "  {'pos': 0.005223492723492724, 'neg': 0.003661710037174721}),\n",
       " ('T_NOUN + T_ADV',\n",
       "  {'pos': 0.006860706860706861, 'neg': 0.005408921933085502})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 only positive bigrams:\")\n",
    "get_top_diff(bigrams, 'pos', 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наибольшая разница очевидно наблюдается у самых частотных n-грамм. А чамые частотные n-граммы в нашем случае это состоящие только из POS тэгов. Это нам мало интересного кроме того, что ADJ + NOUN имеет частотность 1.6% в положительных отзывах и только 1.1% в отрицательных.\n",
    "\n",
    "Тогда посмотрим `get_top_scale()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 only positive bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('T_NOUN + L_всегда',\n",
       "  {'pos': 0.0006172037422037422, 'neg': 3.2527881040892195e-05}),\n",
       " ('L_всегда + T_VERB',\n",
       "  {'pos': 0.0004742723492723493, 'neg': 3.2527881040892195e-05}),\n",
       " ('T_NOUN + L_спасибо',\n",
       "  {'pos': 0.00033134095634095636, 'neg': 2.3234200743494423e-05}),\n",
       " ('L_хороший + L_хостинг',\n",
       "  {'pos': 0.00032484407484407486, 'neg': 2.3234200743494423e-05}),\n",
       " ('L_быстро + T_CCONJ',\n",
       "  {'pos': 0.00033783783783783786, 'neg': 2.7881040892193307e-05})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 only positive bigrams:\")\n",
    "get_top_scale(bigrams, 'pos', 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_top_scale()` уже даёт нам n-граммы поинтереснее, с леммами. Это далеко не самые частотные n-граммы, но зато они очень характерны (в предыдущем случае) для положительных отзывов.\n",
    "\n",
    "Посмотрим на остальные результаты применения `get_top_scale()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 only negative bigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('T_VERB + L_деньги',\n",
       "  {'pos': 4.547817047817048e-05, 'neg': 0.00037639405204460967}),\n",
       " ('L_не + L_работать',\n",
       "  {'pos': 7.146569646569647e-05, 'neg': 0.000525092936802974}),\n",
       " ('T_NUM + T_PROPN',\n",
       "  {'pos': 4.547817047817048e-05, 'neg': 0.00029739776951672863}),\n",
       " ('T_NOUN + L_писать',\n",
       "  {'pos': 4.547817047817048e-05, 'neg': 0.00029275092936802973}),\n",
       " ('T_PART + L_работать',\n",
       "  {'pos': 9.095634095634096e-05, 'neg': 0.0005529739776951672})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 only negative bigrams:\")\n",
    "get_top_scale(bigrams, 'neg', 'pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут уже появилась пара лемм \"не работать\"! Которая явно сигналит нам о плохом отзыве. Также видимо люди шаблон \"VERB + деньги\" употребляют исключительно в негативном контексте типа \"потратил деньги\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 only positive trigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('T_NOUN + T_VERB + L_быстро',\n",
       "  {'pos': 0.00016341177098340537, 'neg': 1.4204007423961214e-05}),\n",
       " ('T_NOUN + L_работать + T_ADV',\n",
       "  {'pos': 0.00022677551891574623, 'neg': 2.130601113594182e-05}),\n",
       " ('L_уже + T_NUM + L_год',\n",
       "  {'pos': 0.00016674670508510752, 'neg': 1.6571341994621415e-05}),\n",
       " ('L_за + T_DET + L_время',\n",
       "  {'pos': 0.00010338295715276667, 'neg': 1.1836672853301011e-05}),\n",
       " ('T_PRON + L_очень + T_ADJ',\n",
       "  {'pos': 0.00010004802305106452, 'neg': 1.1836672853301011e-05})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 only positive trigrams:\")\n",
    "get_top_scale(trigrams, 'pos', 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В топе положительных триграмм у нас \"NOUN + VERB + быстро\". В нашем случае отзывы на хостинги так что это даже имеет смысл. \"хостинг/сервер/поддержка работает/отвечает/помогает + быстро\". Также видимо шаблон \"уже + NUM + год\" очень популярен здесь, \"Я пользуюсь этим хостингом уже 6 лет\", сложно представить негативный отзыв с подобным шаблоном. Тут в целом все результаты интересные но каждый я описывать и комментировать не буду. Идём дальше, к триграммам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 only negative trigrams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('T_X + T_X + T_X',\n",
       "  {'pos': 2.334453871191505e-05, 'neg': 0.00017518275822885496}),\n",
       " ('T_NOUN + L_не + L_работать',\n",
       "  {'pos': 2.334453871191505e-05, 'neg': 0.00013493807052763153}),\n",
       " ('L_раз + T_ADP + T_NOUN',\n",
       "  {'pos': 2.0009604610212903e-05, 'neg': 0.0001017953865383887}),\n",
       " ('T_NOUN + T_PART + L_работать',\n",
       "  {'pos': 3.0014406915319353e-05, 'neg': 0.00013967273966895193}),\n",
       " ('T_PRON + L_что + T_NOUN',\n",
       "  {'pos': 1.667467050851075e-05, 'neg': 7.102003711980607e-05})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 only negative trigrams:\")\n",
    "get_top_scale(trigrams, 'neg', 'pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я предположу, что на первом месте \"X + X + X\" (Х - значит \"другое\"). Я предположу, что это мат. На площадке, откуда парсились данные нецензура закрывается символами \"*\", которые, вероятно, stanza не считает за пунктуацию и они не отфильтровываются."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
