{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of method #1\n",
    "- Get sets of words that are unique for positive/negative reviews\n",
    "- Evaluate sentiment of test samples\n",
    "    - Get set of words from sample\n",
    "    - Intersect with positive and negative sets\n",
    "    - Largest intersection is our evaluation\n",
    "### Description of method #2\n",
    "- Calculate normalized frequencies of each word from train sample.\n",
    "    - `Norlamized frequency = count(word) / count(all_words)`\n",
    "    - It is calculated separately for positive and negative sentiments.\n",
    "- Then evaluate sentiment of test sample texts\n",
    "    - `eval = sum(pos_freq[word]) - sum(neg_freq[word])`\n",
    "    - (eval > 0)? then positive else negative\n",
    "### Description of method #3\n",
    "- Combination of methods #1 and #2\n",
    "- Calculate frequencies\n",
    "- Remove words that are present in both positive and negative reviews\n",
    "- Normalize frequencies\n",
    "- Then evaluate sentiment of test sample texts\n",
    "    - `eval = sum(pos_freq[word]) - sum(neg_freq[word])`\n",
    "    - (eval > 0)? then positive else negative\n",
    "### Description of method #4\n",
    "- Different combination of methods #1 and #2\n",
    "- Get sets of words that are unique for positive/negative reviews\n",
    "- Calculate normalized frequencies of each word from train sample.\n",
    "    - `Norlamized frequency = count(word) / count(all_words)`\n",
    "    - It is calculated separately for positive and negative sentiments.\n",
    "- While evaluating sentiment\n",
    "    - If text has common words with sets of unique positive and negative reviews:\n",
    "        - Decide sentiment by amount of common words\n",
    "    - If text has no common words with sets of unique positive and negative reviews\n",
    "        - Decide sentiment with normalized frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 1)) (0.0.1)\n",
      "Requirement already satisfied: requests in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: pandas in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 4)) (2.0.3)\n",
      "Requirement already satisfied: pymorphy2 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 5)) (0.9.1)\n",
      "Requirement already satisfied: nltk in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 6)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from bs4->-r reqirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from requests->-r reqirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from requests->-r reqirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from requests->-r reqirements.txt (line 2)) (2.0.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from requests->-r reqirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pandas->-r reqirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pandas->-r reqirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pandas->-r reqirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pandas->-r reqirements.txt (line 4)) (1.24.4)\n",
      "Requirement already satisfied: docopt>=0.6 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pymorphy2->-r reqirements.txt (line 5)) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pymorphy2->-r reqirements.txt (line 5)) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pymorphy2->-r reqirements.txt (line 5)) (0.7.2)\n",
      "Requirement already satisfied: click in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from nltk->-r reqirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from nltk->-r reqirements.txt (line 6)) (2023.8.8)\n",
      "Requirement already satisfied: joblib in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from nltk->-r reqirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from scikit-learn->-r reqirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from scikit-learn->-r reqirements.txt (line 7)) (1.10.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from beautifulsoup4->bs4->-r reqirements.txt (line 1)) (2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->-r reqirements.txt (line 4)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r reqirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing reviews\n",
    "We parse hosting reviews from https://hosting101.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawler is a local module that parses target website\n",
    "import crawler\n",
    "import random\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if we alreade have parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_FILE = Path('./data/reviews.json')\n",
    "SKIP_PARSING = DATA_FILE.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing is skipped since parsed file already exists. Delete data/reviews.json if you want to parse new data.\n"
     ]
    }
   ],
   "source": [
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    hosts = crawler.get_hosts(1000)\n",
    "    print(len(hosts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing is skipped since parsed file already exists. Delete data/reviews.json if you want to parse new data.\n"
     ]
    }
   ],
   "source": [
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    reviews = {\n",
    "        'pos': [],\n",
    "        'neg': []\n",
    "    }\n",
    "    pb = tqdm(hosts, position=1)\n",
    "    for i, h in enumerate(pb):\n",
    "        reviews['pos'] += crawler.parse_host(h, positive=True, delay=(1.5, 3), pb=pb)\n",
    "        reviews['neg'] += crawler.parse_host(h, positive=False, delay=(1.5, 3), pb=pb)\n",
    "        if i % 10 == 0:\n",
    "            with open(DATA_FILE, 'w') as f:\n",
    "                json.dump(reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing is skipped since parsed file already exists. Delete data/reviews.json if you want to parse new data.\n"
     ]
    }
   ],
   "source": [
    "# saving results to csv\n",
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    with open(DATA_FILE, 'w') as f:\n",
    "        json.dump(reviews, f)\n",
    "    print(f'Saved parsed data to {DATA_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from csv\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "pos, neg = data[\"pos\"], data[\"neg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into test and reference (\"train\") groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pos: test=560, train=3168; Neg: test=983, train=5565'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling lists before splitting into\n",
    "test_rat = 0.15\n",
    "random.shuffle(pos)\n",
    "random.shuffle(neg)\n",
    "# Splitting samples\n",
    "split_pos_idx = int(len(pos) * (1 - test_rat))\n",
    "split_neg_idx = int(len(neg) * (1 - test_rat))\n",
    "pos_train, pos_test = pos[:split_pos_idx], pos[split_pos_idx:]\n",
    "neg_train, neg_test = neg[:split_neg_idx], neg[split_neg_idx:]\n",
    "del pos, neg, data\n",
    "f\"Pos: test={len(pos_test)}, train={len(pos_train)}; Neg: test={len(neg_test)}, train={len(neg_train)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train group processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kesha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kesha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from typing import List, Tuple, Dict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3eaf188a004b7da98359bd2e92e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b67b89caca44c8ad36a3db9a9f446f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('спасибо очень рад воспользоваться продолжать пользоваться ваш услуга',\n",
       " 'полгода положительный сторона периодически хостинг жёстко глючить сначала вместо страница')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "stoplist = set(stopwords.words('russian'))\n",
    "def tokenizing_pipeline(text: str) -> List[str]:\n",
    "    # Tokenization\n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower(), language=\"russian\")\n",
    "    # removing punctuation and some stopwords to reduce amount of words to parse\n",
    "    tokens = filter(lambda x: x.isalpha() and not x in stoplist, tokens)\n",
    "    # Stemming words\n",
    "    stems = map(lambda x: morph.parse(x)[0].normal_form, tokens)\n",
    "    # Removing remaining stopwords that werent in normal form\n",
    "    stems = filter(lambda x: not x in stoplist, stems)\n",
    "    return list(stems)\n",
    "\n",
    "pos_train = [ tokenizing_pipeline(x) for x in tqdm(pos_train) ]\n",
    "neg_train = [ tokenizing_pipeline(x) for x in tqdm(neg_train) ]\n",
    "\n",
    "' '.join(random.choice(pos_train)[:10]), ' '.join(random.choice(neg_train)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting unique words / frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 285)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "from collections import Counter\n",
    "minimum_freq = 75\n",
    "\n",
    "def form_set(arr: List[List[str]]) -> set:\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    for k, v in list(counter.items()):\n",
    "        if v < minimum_freq:\n",
    "            del counter[k]\n",
    "    return set(counter.keys())\n",
    "\n",
    "# words for positive and negative comments\n",
    "pos_set = form_set(pos_train)\n",
    "neg_set = form_set(neg_train)\n",
    "# their intersection\n",
    "intersection = pos_set.intersection(neg_set)\n",
    "# unique words for positive and negative that are not present in other category\n",
    "pos_unique = pos_set.difference(intersection)\n",
    "neg_unique = neg_set.difference(intersection)\n",
    "del pos_set, neg_set\n",
    "len(pos_unique), len(neg_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: pos=164, neg=411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('дело', 0.0026163909195844556),\n",
       "  ('бывать', 0.002898550724637681),\n",
       "  ('техподдержка', 0.012568936770552777)],\n",
       " [('сайт', 0.05429903648957164),\n",
       "  ('лежать', 0.005852648686298031),\n",
       "  ('час', 0.007061873621483574)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method 2\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_normalized_freq(arr: List[List[str]]) -> defaultdict:\n",
    "    \"\"\"Counts normalized word frequency from all texts in given list.\n",
    "    Normalized means a float number [0, 1] = word_freq / total_words \"\"\"\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    # Filtering out low frequest words\n",
    "    freq_list = { k: v for k, v in counter.items() if v >= minimum_freq }\n",
    "    # Counting total words\n",
    "    total_words = sum(freq_list.values())\n",
    "    # Calculating normalized freq\n",
    "    for k, v in freq_list.items():\n",
    "        freq_list[k] = v / total_words\n",
    "    return defaultdict(float, freq_list)\n",
    "\n",
    "pos_normalized_freq = count_normalized_freq(pos_train)\n",
    "neg_normalized_freq = count_normalized_freq(neg_train)\n",
    "print(f'Total words: pos={len(pos_normalized_freq)}, neg={len(neg_normalized_freq)}')\n",
    "list(pos_normalized_freq.items())[:3], list(neg_normalized_freq.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: pos=164, neg=411\n",
      "Total unique words: pos=38, neg=285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('отличный', 0.107808652831516),\n",
       "  ('спасибо', 0.07720717551881814),\n",
       "  ('довольный', 0.0661273302849103),\n",
       "  ('помогать', 0.05170594442490327),\n",
       "  ('быстрый', 0.04396763981709462),\n",
       "  ('оперативно', 0.03781217024270137),\n",
       "  ('удобный', 0.033943017938797046),\n",
       "  ('устраивать', 0.03218431234611326),\n",
       "  ('отлично', 0.029370383397819205),\n",
       "  ('радовать', 0.02761167780513542)],\n",
       " [('лежать', 0.01451117720425981),\n",
       "  ('письмо', 0.013000095941667466),\n",
       "  ('сутки', 0.012664300105535833),\n",
       "  ('аккаунт', 0.012280533435671112),\n",
       "  ('плохой', 0.011680898014007483),\n",
       "  ('постоянно', 0.010049889667082415),\n",
       "  ('ошибка', 0.010001918833349324),\n",
       "  ('неделя', 0.009881991749016598),\n",
       "  ('оплатить', 0.009858006332150053),\n",
       "  ('почта', 0.009786050081550417)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 3\n",
    "def count_freq(arr: List[List[str]]) -> defaultdict:\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    # Filtering out low frequest words\n",
    "    freq_list = filter(lambda x: x[1] >= minimum_freq, counter.items())\n",
    "    return defaultdict(float, { k: v for k, v in freq_list })\n",
    "# Calculating frequencies\n",
    "pos_freq = count_freq(pos_train)\n",
    "neg_freq = count_freq(neg_train)\n",
    "print(f'Total words: pos={len(pos_freq)}, neg={len(neg_freq)}')\n",
    "# Calculating words that are present in both sets\n",
    "intersection = set(pos_freq.keys()).intersection(set(neg_freq.keys()))\n",
    "# Removing words that are present in other set\n",
    "for k in intersection:\n",
    "    del pos_freq[k], neg_freq[k]\n",
    "# Calculating normalized freq\n",
    "def normalize_freq(freq_dict: dict):\n",
    "    total_words = sum(freq_dict.values())\n",
    "    for k, v in freq_dict.items():\n",
    "        freq_dict[k] = v / total_words\n",
    "normalize_freq(pos_freq)\n",
    "normalize_freq(neg_freq)\n",
    "pos_normalized_unique_freq = pos_freq\n",
    "neg_normalized_unique_freq = neg_freq\n",
    "print(f'Total unique words: pos={len(pos_freq)}, neg={len(neg_freq)}')\n",
    "sorted(list(pos_normalized_unique_freq.items()), key=lambda x: x[1], reverse=True)[:10], sorted(list(neg_normalized_unique_freq.items()), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word amount (with test sample size = 0.15)\n",
    "|minimum_freq|total words|total unique* words|\n",
    "|-|-|-|\n",
    "|1|pos=7102, neg=13476|pos=2505, neg=8879|\n",
    "|2|pos=3388, neg=6630|pos=688, neg=3930|\n",
    "|3|pos=2426, neg=4959|pos=367, neg=2900|\n",
    "|5|pos=1658, neg=3506|pos=206, neg=2054|\n",
    "|10|pos=1018, neg=2233|pos=110, neg=1325|\n",
    "|50|pos=258, neg=604|pos=43, neg=389|\n",
    "|100|pos=113, neg=307|pos=25, neg=219|\n",
    "\n",
    "\\* Unique words - words that are not present in another group. Words exept the intersection of pos, neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #1\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.67 |\n",
    "| 2 | 0.69 |\n",
    "| 3 | 0.68 |\n",
    "| 5 | 0.70 |\n",
    "| 10 | 0.71 |\n",
    "| 50 | 0.73 |\n",
    "| 100 | 0.73 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f58ce999e4d45bdaa63db45b6c6528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7623ad27ba0a4a6c9b3e2db7cd35cbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7680\n"
     ]
    }
   ],
   "source": [
    "def eval_sentiment_1(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    tokens_unique = set(tokens)\n",
    "    sent_eval = 0\n",
    "    sent_eval += len(tokens_unique.intersection(pos_unique))\n",
    "    sent_eval -= len(tokens_unique.intersection(neg_unique))\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_1(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_1(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #2\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.62 |\n",
    "| 2 | 0.63 |\n",
    "| 3 | 0.61 |\n",
    "| 5 | 0.61 |\n",
    "| 10 | 0.60 |\n",
    "| 50 | 0.58 |\n",
    "| 100 | 0.56 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32209e54581f4e6b9d73b3d2e6e792b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d88d876344b4c2391b2590ae10ec414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5762\n"
     ]
    }
   ],
   "source": [
    "def eval_sentiment_2(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    sent_eval = 0\n",
    "    for tkn in tokens:\n",
    "        sent_eval += pos_normalized_freq[tkn] - neg_normalized_freq[tkn]\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_2(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_2(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #3\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.68 |\n",
    "| 2 | 0.70 |\n",
    "| 3 | 0.70 |\n",
    "| 5 | 0.72 |\n",
    "| 10 | 0.74 |\n",
    "| 50 | 0.81 |\n",
    "| 100 | 0.83 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ecb886e9f04a668ca7750f50709147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89ee74ff9904f48a2dd8c3acb237f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8360\n"
     ]
    }
   ],
   "source": [
    "def eval_sentiment_3(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    sent_eval = 0\n",
    "    for tkn in tokens:\n",
    "        sent_eval += pos_normalized_unique_freq[tkn] - neg_normalized_unique_freq[tkn]\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method #4\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.77 |\n",
    "| 2 | 0.80 |\n",
    "| 3 | 0.80 |\n",
    "| 5 | 0.80 |\n",
    "| 10 | 0.77 |\n",
    "| 50 | 0.74 |\n",
    "| 100 | 0.74 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dea5798b8bd442893a4c7a4332d02b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736ba6aefa2143f3bfe9e4229b6dca29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7693\n"
     ]
    }
   ],
   "source": [
    "def eval_sentiment_3(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    tokens_unique = set(tokens)\n",
    "    sent_eval = 0\n",
    "    pos_eval = len(tokens_unique.intersection(pos_unique))\n",
    "    neg_eval = len(tokens_unique.intersection(neg_unique))\n",
    "    if pos_eval == 0 and neg_eval == 0:\n",
    "        for tkn in tokens:\n",
    "            sent_eval += pos_normalized_freq[tkn] - neg_normalized_freq[tkn]\n",
    "    else:\n",
    "        sent_eval = pos_eval - neg_eval\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performances\n",
    "- accuracy=0.83 - method #3 - minimum_freq=100\n",
    "- accuracy=0.80 - method #4 - minimum_freq=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answer = random.choice([1, -1])\n",
    "if correct_answer == 1:\n",
    "    review = random.choice(pos_test)\n",
    "else:\n",
    "    review = random.choice(neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "Опять какой то мальчик-админ сидит и аккаунты сортитирует по своему усмотрению. Только что опять перенесли на другой IP кучу сайтов. В результате -полдня сайты не работали  - деньги на рекламу за это время -в трубу. Но провайдеру на это наплевать. Зачем делать дополнительные телодвижения и уведомлять пользователей о смене IP. Для этого, минимум, надо уважать своих клиентов, а это не в трендах Таймвеба. Для таких оповещений даже никаких затрат не требуется. \n",
      "##########\n",
      "Unique positive words: \n",
      "Unique negatve words: реклама ip пользователь куча аккаунт сидеть результат\n",
      "Eval = -7\n",
      "Answer: Negative | Correct\n"
     ]
    }
   ],
   "source": [
    "def explain_sentiment_eval_1(text):\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    tokens_unique = set(tokens)\n",
    "    sent_eval = 0\n",
    "    print(f\"Unique positive words: {' '.join(tokens_unique.intersection(pos_unique))}\")\n",
    "    print(f\"Unique negatve words: {' '.join(tokens_unique.intersection(neg_unique))}\")\n",
    "    sent_eval += len(tokens_unique.intersection(pos_unique))\n",
    "    sent_eval -= len(tokens_unique.intersection(neg_unique))\n",
    "    print(f\"Eval = {sent_eval}\")\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "print('#' * 10)\n",
    "print(review)\n",
    "print('#' * 10)\n",
    "answer = explain_sentiment_eval_1(review)\n",
    "print(\"Answer:\", \"Positive\" if answer == 1 else \"Negative\", \"|\", \"Correct\" if answer == correct_answer else \"Incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "Опять какой то мальчик-админ сидит и аккаунты сортитирует по своему усмотрению. Только что опять перенесли на другой IP кучу сайтов. В результате -полдня сайты не работали  - деньги на рекламу за это время -в трубу. Но провайдеру на это наплевать. Зачем делать дополнительные телодвижения и уведомлять пользователей о смене IP. Для этого, минимум, надо уважать своих клиентов, а это не в трендах Таймвеба. Для таких оповещений даже никаких затрат не требуется. \n",
      "##########\n",
      "'сидеть'[+0.000|-0.001] 'аккаунт'[+0.000|-0.005] 'свой'[+0.011|-0.009] 'перенести'[+0.004|-0.003] 'ip'[+0.000|-0.002] 'куча'[+0.000|-0.001] 'сайт'[+0.049|-0.054] 'результат'[+0.000|-0.001] 'сайт'[+0.049|-0.054] 'работать'[+0.027|-0.016] 'деньга'[+0.003|-0.011] 'реклама'[+0.000|-0.001] 'это'[+0.019|-0.020] 'время'[+0.010|-0.008] 'провайдер'[+0.004|-0.003] 'это'[+0.019|-0.020] 'делать'[+0.003|-0.004] 'пользователь'[+0.000|-0.002] 'ip'[+0.000|-0.002] 'свой'[+0.011|-0.009] 'клиент'[+0.007|-0.010] 'это'[+0.019|-0.020] 'никакой'[+0.005|-0.004] \n",
      "Eval = -0.019267473550626246\n",
      "Answer: Negative | Correct\n"
     ]
    }
   ],
   "source": [
    "def explain_sentiment_eval_2(text):\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    sent_eval = 0\n",
    "    for tkn in tokens:\n",
    "        sent_eval += pos_normalized_freq[tkn] - neg_normalized_freq[tkn]\n",
    "        if pos_normalized_freq[tkn] - neg_normalized_freq[tkn] != 0:\n",
    "            print(f\"'{tkn}'[+{pos_normalized_freq[tkn]:.3f}|-{neg_normalized_freq[tkn]:.3f}]\", end=' ')\n",
    "    print(f\"\\nEval = {sent_eval}\")\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "print('#' * 10)\n",
    "print(review)\n",
    "print('#' * 10)\n",
    "answer = explain_sentiment_eval_2(review)\n",
    "print(\"Answer:\", \"Positive\" if answer == 1 else \"Negative\", \"|\", \"Correct\" if answer == correct_answer else \"Incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "Опять какой то мальчик-админ сидит и аккаунты сортитирует по своему усмотрению. Только что опять перенесли на другой IP кучу сайтов. В результате -полдня сайты не работали  - деньги на рекламу за это время -в трубу. Но провайдеру на это наплевать. Зачем делать дополнительные телодвижения и уведомлять пользователей о смене IP. Для этого, минимум, надо уважать своих клиентов, а это не в трендах Таймвеба. Для таких оповещений даже никаких затрат не требуется. \n",
      "##########\n",
      "'сидеть'[+0.000|-0.003] 'аккаунт'[+0.000|-0.012] 'ip'[+0.000|-0.005] 'куча'[+0.000|-0.002] 'результат'[+0.000|-0.003] 'реклама'[+0.000|-0.002] 'пользователь'[+0.000|-0.004] 'ip'[+0.000|-0.005] \n",
      "Eval = -0.03691355655761297\n",
      "Answer: Negative | Correct\n"
     ]
    }
   ],
   "source": [
    "def explain_sentiment_eval_3(text):\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    sent_eval = 0\n",
    "    for tkn in tokens:\n",
    "        sent_eval += pos_normalized_unique_freq[tkn] - neg_normalized_unique_freq[tkn]\n",
    "        if pos_normalized_unique_freq[tkn] - neg_normalized_unique_freq[tkn] != 0:\n",
    "            print(f\"'{tkn}'[+{pos_normalized_unique_freq[tkn]:.3f}|-{neg_normalized_unique_freq[tkn]:.3f}]\", end=' ')\n",
    "    print(f\"\\nEval = {sent_eval}\")\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "print('#' * 10)\n",
    "print(review)\n",
    "print('#' * 10)\n",
    "answer = explain_sentiment_eval_3(review)\n",
    "print(\"Answer:\", \"Positive\" if answer == 1 else \"Negative\", \"|\", \"Correct\" if answer == correct_answer else \"Incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "Опять какой то мальчик-админ сидит и аккаунты сортитирует по своему усмотрению. Только что опять перенесли на другой IP кучу сайтов. В результате -полдня сайты не работали  - деньги на рекламу за это время -в трубу. Но провайдеру на это наплевать. Зачем делать дополнительные телодвижения и уведомлять пользователей о смене IP. Для этого, минимум, надо уважать своих клиентов, а это не в трендах Таймвеба. Для таких оповещений даже никаких затрат не требуется. \n",
      "##########\n",
      "Unique positive words: \n",
      "Unique negatve words: реклама ip пользователь куча аккаунт сидеть результат\n",
      "Eval = -7\n",
      "Answer: Negative | Correct\n"
     ]
    }
   ],
   "source": [
    "def explain_sentiment_eval_4(text):\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    tokens_unique = set(tokens)\n",
    "    sent_eval = 0\n",
    "    pos_words = tokens_unique.intersection(pos_unique)\n",
    "    neg_words = tokens_unique.intersection(neg_unique)\n",
    "    if len(pos_words) == 0 and len(neg_words) == 0:\n",
    "        print(f'No unique words found. Comparing to non unique normalized frequency')\n",
    "        for tkn in tokens:\n",
    "            sent_eval += pos_normalized_freq[tkn] - neg_normalized_freq[tkn]\n",
    "            if pos_normalized_freq[tkn] - neg_normalized_freq[tkn] != 0:\n",
    "                print(f\"'{tkn}'[+{pos_normalized_freq[tkn]:.3f}|-{neg_normalized_freq[tkn]:.3f}]\", end=' ')\n",
    "        print(f\"\\nEval = {sent_eval}\")\n",
    "    else:\n",
    "        print(f\"Unique positive words: {' '.join(pos_words)}\")\n",
    "        print(f\"Unique negatve words: {' '.join(neg_words)}\")\n",
    "        sent_eval = len(pos_words) - len(neg_words)\n",
    "        print(f\"Eval = {sent_eval}\")\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "print('#' * 10)\n",
    "print(review)\n",
    "print('#' * 10)\n",
    "answer = explain_sentiment_eval_4(review)\n",
    "print(\"Answer:\", \"Positive\" if answer == 1 else \"Negative\", \"|\", \"Correct\" if answer == correct_answer else \"Incorrect\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
