{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of method #1\n",
    "- Get sets of words that are unique for positive/negative reviews\n",
    "- Evaluate sentiment of test samples\n",
    "    - Get set of words from sample\n",
    "    - Intersect with positive and negative sets\n",
    "    - Largest intersection is our evaluation\n",
    "### Description of method #2\n",
    "- Calculate normalized frequencies of each word from train sample.\n",
    "    - `Norlamized frequency = count(word) / count(all_words)`\n",
    "    - It is calculated separately for positive and negative sentiments.\n",
    "- Then evaluate sentiment of test sample texts\n",
    "    - `eval = sum(pos_freq[word]) - sum(neg_freq[word])`\n",
    "    - (eval > 0)? then positive else negative\n",
    "### Description of method #3\n",
    "- Combination of methods #1 and #2\n",
    "- Calculate frequencies\n",
    "- Remove words that are present in both positive and negative reviews\n",
    "- Normalize frequencies\n",
    "- Then evaluate sentiment of test sample texts\n",
    "    - `eval = sum(pos_freq[word]) - sum(neg_freq[word])`\n",
    "    - (eval > 0)? then positive else negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r reqirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing reviews\n",
    "We parse hosting reviews from https://hosting101.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawler is a local module that parses target website\n",
    "import crawler\n",
    "import random\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if we alreade have parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_FILE = Path('./data/reviews.json')\n",
    "SKIP_PARSING = DATA_FILE.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    hosts = crawler.get_hosts(1000)\n",
    "    print(len(hosts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    reviews = {\n",
    "        'pos': [],\n",
    "        'neg': []\n",
    "    }\n",
    "    pb = tqdm(hosts, position=1)\n",
    "    for i, h in enumerate(pb):\n",
    "        reviews['pos'] += crawler.parse_host(h, positive=True, delay=(1.5, 3), pb=pb)\n",
    "        reviews['neg'] += crawler.parse_host(h, positive=False, delay=(1.5, 3), pb=pb)\n",
    "        if i % 10 == 0:\n",
    "            with open(DATA_FILE, 'w') as f:\n",
    "                json.dump(reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results to csv\n",
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    with open(DATA_FILE, 'w') as f:\n",
    "        json.dump(reviews, f)\n",
    "    print(f'Saved parsed data to {DATA_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from csv\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "pos, neg = data[\"pos\"], data[\"neg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into test and reference (\"train\") groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling lists before splitting into\n",
    "test_rat = 0.15\n",
    "random.shuffle(pos)\n",
    "random.shuffle(neg)\n",
    "# Splitting samples\n",
    "split_pos_idx = int(len(pos) * (1 - test_rat))\n",
    "split_neg_idx = int(len(neg) * (1 - test_rat))\n",
    "pos_train, pos_test = pos[:split_pos_idx], pos[split_pos_idx:]\n",
    "neg_train, neg_test = neg[:split_neg_idx], neg[split_neg_idx:]\n",
    "del pos, neg, data\n",
    "f\"Pos: test={len(pos_test)}, train={len(pos_train)}; Neg: test={len(neg_test)}, train={len(neg_train)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train group processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "stoplist = set(stopwords.words('russian'))\n",
    "def tokenizing_pipeline(text: str) -> List[str]:\n",
    "    # Tokenization\n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower(), language=\"russian\")\n",
    "    # removing punctuation and some stopwords to reduce amount of words to parse\n",
    "    tokens = filter(lambda x: x.isalpha() and not x in stoplist, tokens)\n",
    "    # Stemming words\n",
    "    stems = map(lambda x: morph.parse(x)[0].normal_form, tokens)\n",
    "    # Removing remaining stopwords that werent in normal form\n",
    "    stems = filter(lambda x: not x in stoplist, stems)\n",
    "    return list(stems)\n",
    "\n",
    "pos_train = [ tokenizing_pipeline(x) for x in tqdm(pos_train) ]\n",
    "neg_train = [ tokenizing_pipeline(x) for x in tqdm(neg_train) ]\n",
    "\n",
    "' '.join(random.choice(pos_train)[:10]), ' '.join(random.choice(neg_train)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting unique words / frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "from collections import Counter\n",
    "minimum_freq = 150\n",
    "\n",
    "def form_set(arr: List[List[str]]) -> set:\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    for k, v in list(counter.items()):\n",
    "        if v < minimum_freq:\n",
    "            del counter[k]\n",
    "    return set(counter.keys())\n",
    "\n",
    "# words for positive and negative comments\n",
    "pos_set = form_set(pos_train)\n",
    "neg_set = form_set(neg_train)\n",
    "# their intersection\n",
    "intersection = pos_set.intersection(neg_set)\n",
    "# unique words for positive and negative that are not present in other category\n",
    "pos_unique = pos_set.difference(intersection)\n",
    "neg_unique = neg_set.difference(intersection)\n",
    "del pos_set, neg_set\n",
    "len(pos_unique), len(neg_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_normalized_freq(arr: List[List[str]]) -> defaultdict:\n",
    "    \"\"\"Counts normalized word frequency from all texts in given list.\n",
    "    Normalized means a float number [0, 1] = word_freq / total_words \"\"\"\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    # Filtering out low frequest words\n",
    "    freq_list = { k: v for k, v in counter.items() if v >= minimum_freq }\n",
    "    # Counting total words\n",
    "    total_words = sum(freq_list.values())\n",
    "    # Calculating normalized freq\n",
    "    for k, v in freq_list.items():\n",
    "        freq_list[k] = v / total_words\n",
    "    return defaultdict(float, freq_list)\n",
    "\n",
    "pos_normalized_freq = count_normalized_freq(pos_train)\n",
    "neg_normalized_freq = count_normalized_freq(neg_train)\n",
    "print(f'Total words: pos={len(pos_normalized_freq)}, neg={len(neg_normalized_freq)}')\n",
    "list(pos_normalized_freq.items())[:3], list(neg_normalized_freq.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3\n",
    "def count_freq(arr: List[List[str]]) -> defaultdict:\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    # Filtering out low frequest words\n",
    "    freq_list = filter(lambda x: x[1] >= minimum_freq, counter.items())\n",
    "    return defaultdict(float, { k: v for k, v in freq_list })\n",
    "# Calculating frequencies\n",
    "pos_freq = count_freq(pos_train)\n",
    "neg_freq = count_freq(neg_train)\n",
    "print(f'Total words: pos={len(pos_freq)}, neg={len(neg_freq)}')\n",
    "# Calculating words that are present in both sets\n",
    "intersection = set(pos_freq.keys()).intersection(set(neg_freq.keys()))\n",
    "# Removing words that are present in other set\n",
    "for k in intersection:\n",
    "    del pos_freq[k], neg_freq[k]\n",
    "# Calculating normalized freq\n",
    "def normalize_freq(freq_dict: dict):\n",
    "    total_words = sum(freq_dict.values())\n",
    "    for k, v in freq_dict.items():\n",
    "        freq_dict[k] = v / total_words\n",
    "normalize_freq(pos_freq)\n",
    "normalize_freq(neg_freq)\n",
    "pos_normalized_unique_freq = pos_freq\n",
    "neg_normalized_unique_freq = neg_freq\n",
    "print(f'Total unique words: pos={len(pos_freq)}, neg={len(neg_freq)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word amount\n",
    "|minimum_freq|total words|total unique* words|\n",
    "|-|-|-|\n",
    "|1|pos=7102, neg=13476|pos=2505, neg=8879|\n",
    "|2|pos=3388, neg=6630|pos=688, neg=3930|\n",
    "|3|pos=2426, neg=4959|pos=367, neg=2900|\n",
    "|5|pos=1658, neg=3506|pos=206, neg=2054|\n",
    "|10|pos=1018, neg=2233|pos=110, neg=1325|\n",
    "|50|pos=258, neg=604|pos=43, neg=389|\n",
    "|100|pos=113, neg=307|pos=25, neg=219|\n",
    "\n",
    "\\* Unique words - words that are not present in another group. Words exept the intersection of pos, neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #1\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.67 |\n",
    "| 2 | 0.69 |\n",
    "| 3 | 0.68 |\n",
    "| 5 | 0.70 |\n",
    "| 10 | 0.71 |\n",
    "| 50 | 0.73 |\n",
    "| 100 | 0.73 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sentiment_1(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    tokens_unique = set(tokens)\n",
    "    sent_eval = 0\n",
    "    sent_eval += len(tokens_unique.intersection(pos_unique))\n",
    "    sent_eval -= len(tokens_unique.intersection(neg_unique))\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_1(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_1(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #2\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.62 |\n",
    "| 2 | 0.63 |\n",
    "| 3 | 0.61 |\n",
    "| 5 | 0.61 |\n",
    "| 10 | 0.60 |\n",
    "| 50 | 0.58 |\n",
    "| 100 | 0.56 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_sentiment_2(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    sent_eval = 0\n",
    "    for tkn in tokens:\n",
    "        sent_eval += pos_normalized_freq[tkn] - neg_normalized_freq[tkn]\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_2(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_2(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #3\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.68 |\n",
    "| 2 | 0.70 |\n",
    "| 3 | 0.70 |\n",
    "| 5 | 0.72 |\n",
    "| 10 | 0.74 |\n",
    "| 50 | 0.81 |\n",
    "| 100 | 0.83 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_sentiment_3(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    sent_eval = 0\n",
    "    for tkn in tokens:\n",
    "        sent_eval += pos_normalized_unique_freq[tkn] - neg_normalized_unique_freq[tkn]\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method #4\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.77 |\n",
    "| 2 | 0.80 |\n",
    "| 3 | 0.79 |\n",
    "| 5 | 0.80 |\n",
    "| 10 | 0.77 |\n",
    "| 50 | 0.74 |\n",
    "| 100 | 0.74 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sentiment_3(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    tokens_unique = set(tokens)\n",
    "    sent_eval = 0\n",
    "    pos_eval = len(tokens_unique.intersection(pos_unique))\n",
    "    neg_eval = len(tokens_unique.intersection(neg_unique))\n",
    "    if pos_eval == 0 and neg_eval == 0:\n",
    "        for tkn in tokens:\n",
    "            sent_eval += pos_normalized_freq[tkn] - neg_normalized_freq[tkn]\n",
    "    else:\n",
    "        sent_eval = pos_eval - neg_eval\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performances\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
