{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of method #1\n",
    "- Get sets of words that are unique for positive/negative reviews\n",
    "- Evaluate sentiment of test samples\n",
    "    - Get set of words from sample\n",
    "    - Intersect with positive and negative sets\n",
    "    - Largest intersection is our evaluation\n",
    "### Description of method #2\n",
    "- Calculate normalized frequencies of each word from train sample.\n",
    "    - `Norlamized frequency = count(word) / count(all_words)`\n",
    "    - It is calculated separately for positive and negative sentiments.\n",
    "- Then evaluate sentiment of test sample texts\n",
    "    - `eval = sum(pos_freq[word]) - sum(neg_freq[word])`\n",
    "    - (eval > 0)? then positive else negative\n",
    "### Description of method #3\n",
    "- Combination of methods #1 and #2\n",
    "- Calculate frequencies\n",
    "- Remove words that are present in both positive and negative reviews\n",
    "- Normalize frequencies\n",
    "- Then evaluate sentiment of test sample texts\n",
    "    - `eval = sum(pos_freq[word]) - sum(neg_freq[word])`\n",
    "    - (eval > 0)? then positive else negative\n",
    "### Description of method #4\n",
    "- Different combination of methods #1 and #2\n",
    "- Get sets of words that are unique for positive/negative reviews\n",
    "- Calculate normalized frequencies of each word from train sample.\n",
    "    - `Norlamized frequency = count(word) / count(all_words)`\n",
    "    - It is calculated separately for positive and negative sentiments.\n",
    "- While evaluating sentiment\n",
    "    - If text has common words with sets of unique positive and negative reviews:\n",
    "        - Decide sentiment by amount of common words\n",
    "    - If text has no common words with sets of unique positive and negative reviews\n",
    "        - Decide sentiment with normalized frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 1)) (0.0.1)\n",
      "Requirement already satisfied: requests in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: pandas in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 4)) (2.0.3)\n",
      "Requirement already satisfied: pymorphy2 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 5)) (0.9.1)\n",
      "Requirement already satisfied: nltk in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 6)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from -r reqirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from bs4->-r reqirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from requests->-r reqirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from requests->-r reqirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from requests->-r reqirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from requests->-r reqirements.txt (line 2)) (2.0.5)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pandas->-r reqirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pandas->-r reqirements.txt (line 4)) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pandas->-r reqirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pandas->-r reqirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pymorphy2->-r reqirements.txt (line 5)) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pymorphy2->-r reqirements.txt (line 5)) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from pymorphy2->-r reqirements.txt (line 5)) (2.4.417127.4579844)\n",
      "Requirement already satisfied: joblib in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from nltk->-r reqirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: click in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from nltk->-r reqirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from nltk->-r reqirements.txt (line 6)) (2023.8.8)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from scikit-learn->-r reqirements.txt (line 7)) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from scikit-learn->-r reqirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from beautifulsoup4->bs4->-r reqirements.txt (line 1)) (2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/kesha/jupyter_venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->-r reqirements.txt (line 4)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r reqirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing reviews\n",
    "We parse hosting reviews from https://hosting101.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawler is a local module that parses target website\n",
    "import crawler\n",
    "import random\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if we alreade have parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_FILE = Path('./data/reviews.json')\n",
    "SKIP_PARSING = DATA_FILE.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing is skipped since parsed file already exists. Delete data/reviews.json if you want to parse new data.\n"
     ]
    }
   ],
   "source": [
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    hosts = crawler.get_hosts(1000)\n",
    "    print(len(hosts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing is skipped since parsed file already exists. Delete data/reviews.json if you want to parse new data.\n"
     ]
    }
   ],
   "source": [
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    reviews = {\n",
    "        'pos': [],\n",
    "        'neg': []\n",
    "    }\n",
    "    pb = tqdm(hosts, position=1)\n",
    "    for i, h in enumerate(pb):\n",
    "        reviews['pos'] += crawler.parse_host(h, positive=True, delay=(1.5, 3), pb=pb)\n",
    "        reviews['neg'] += crawler.parse_host(h, positive=False, delay=(1.5, 3), pb=pb)\n",
    "        if i % 10 == 0:\n",
    "            with open(DATA_FILE, 'w') as f:\n",
    "                json.dump(reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing is skipped since parsed file already exists. Delete data/reviews.json if you want to parse new data.\n"
     ]
    }
   ],
   "source": [
    "# saving results to csv\n",
    "if SKIP_PARSING:\n",
    "    print(f'Parsing is skipped since parsed file already exists. Delete {DATA_FILE} if you want to parse new data.')\n",
    "else:\n",
    "    with open(DATA_FILE, 'w') as f:\n",
    "        json.dump(reviews, f)\n",
    "    print(f'Saved parsed data to {DATA_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from csv\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "pos, neg = data[\"pos\"], data[\"neg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into test and reference (\"train\") groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pos: test=560, train=3168; Neg: test=983, train=5565'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling lists before splitting into\n",
    "test_rat = 0.15\n",
    "random.shuffle(pos)\n",
    "random.shuffle(neg)\n",
    "# Splitting samples\n",
    "split_pos_idx = int(len(pos) * (1 - test_rat))\n",
    "split_neg_idx = int(len(neg) * (1 - test_rat))\n",
    "pos_train, pos_test = pos[:split_pos_idx], pos[split_pos_idx:]\n",
    "neg_train, neg_test = neg[:split_neg_idx], neg[split_neg_idx:]\n",
    "del pos, neg, data\n",
    "f\"Pos: test={len(pos_test)}, train={len(pos_train)}; Neg: test={len(neg_test)}, train={len(neg_train)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train group processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kesha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kesha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from typing import List, Tuple, Dict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e034f1339a4a18b42bc96b3a33baca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc85d6dd697445d93aaa936d93c0b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('изначально опираться хостинг профессиональный адекватный техподдержка угадать технически вообще претензия',\n",
       " 'пользоваться данные хостинг год лето год начаться постоянный проблема доступность')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "stoplist = set(stopwords.words('russian'))\n",
    "def tokenizing_pipeline(text: str) -> List[str]:\n",
    "    # Tokenization\n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower(), language=\"russian\")\n",
    "    # removing punctuation and some stopwords to reduce amount of words to parse\n",
    "    tokens = filter(lambda x: x.isalpha() and not x in stoplist, tokens)\n",
    "    # Stemming words\n",
    "    stems = map(lambda x: morph.parse(x)[0].normal_form, tokens)\n",
    "    # Removing remaining stopwords that werent in normal form\n",
    "    stems = filter(lambda x: not x in stoplist, stems)\n",
    "    return list(stems)\n",
    "\n",
    "pos_train = [ tokenizing_pipeline(x) for x in tqdm(pos_train) ]\n",
    "neg_train = [ tokenizing_pipeline(x) for x in tqdm(neg_train) ]\n",
    "\n",
    "' '.join(random.choice(pos_train)[:10]), ' '.join(random.choice(neg_train)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting unique words / frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 131)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "from collections import Counter\n",
    "minimum_freq = 150\n",
    "\n",
    "def form_set(arr: List[List[str]]) -> set:\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    for k, v in list(counter.items()):\n",
    "        if v < minimum_freq:\n",
    "            del counter[k]\n",
    "    return set(counter.keys())\n",
    "\n",
    "# words for positive and negative comments\n",
    "pos_set = form_set(pos_train)\n",
    "neg_set = form_set(neg_train)\n",
    "# their intersection\n",
    "intersection = pos_set.intersection(neg_set)\n",
    "# unique words for positive and negative that are not present in other category\n",
    "pos_unique = pos_set.difference(intersection)\n",
    "neg_unique = neg_set.difference(intersection)\n",
    "del pos_set, neg_set\n",
    "len(pos_unique), len(neg_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: pos=69, neg=187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('это', 0.025531482046587325),\n",
       "  ('первый', 0.005289390702878649),\n",
       "  ('мочь', 0.007595022547723189)],\n",
       " [('сайт', 0.07059653187925802),\n",
       "  ('висеть', 0.001888954652495309),\n",
       "  ('ошибка', 0.005226107871903689)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method 2\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_normalized_freq(arr: List[List[str]]) -> defaultdict:\n",
    "    \"\"\"Counts normalized word frequency from all texts in given list.\n",
    "    Normalized means a float number [0, 1] = word_freq / total_words \"\"\"\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    # Filtering out low frequest words\n",
    "    freq_list = { k: v for k, v in counter.items() if v >= minimum_freq }\n",
    "    # Counting total words\n",
    "    total_words = sum(freq_list.values())\n",
    "    # Calculating normalized freq\n",
    "    for k, v in freq_list.items():\n",
    "        freq_list[k] = v / total_words\n",
    "    return defaultdict(float, freq_list)\n",
    "\n",
    "pos_normalized_freq = count_normalized_freq(pos_train)\n",
    "neg_normalized_freq = count_normalized_freq(neg_train)\n",
    "print(f'Total words: pos={len(pos_normalized_freq)}, neg={len(neg_normalized_freq)}')\n",
    "list(pos_normalized_freq.items())[:3], list(neg_normalized_freq.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: pos=69, neg=187\n",
      "Total unique words: pos=13, neg=131\n"
     ]
    }
   ],
   "source": [
    "# Method 3\n",
    "def count_freq(arr: List[List[str]]) -> defaultdict:\n",
    "    counter = Counter()\n",
    "    for text in arr:\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    # Filtering out low frequest words\n",
    "    freq_list = filter(lambda x: x[1] >= minimum_freq, counter.items())\n",
    "    return defaultdict(float, { k: v for k, v in freq_list })\n",
    "# Calculating frequencies\n",
    "pos_freq = count_freq(pos_train)\n",
    "neg_freq = count_freq(neg_train)\n",
    "print(f'Total words: pos={len(pos_freq)}, neg={len(neg_freq)}')\n",
    "# Calculating words that are present in both sets\n",
    "intersection = set(pos_freq.keys()).intersection(set(neg_freq.keys()))\n",
    "# Removing words that are present in other set\n",
    "for k in intersection:\n",
    "    del pos_freq[k], neg_freq[k]\n",
    "# Calculating normalized freq\n",
    "def normalize_freq(freq_dict: dict):\n",
    "    total_words = sum(freq_dict.values())\n",
    "    for k, v in freq_dict.items():\n",
    "        freq_dict[k] = v / total_words\n",
    "normalize_freq(pos_freq)\n",
    "normalize_freq(neg_freq)\n",
    "pos_normalized_unique_freq = pos_freq\n",
    "neg_normalized_unique_freq = neg_freq\n",
    "print(f'Total unique words: pos={len(pos_freq)}, neg={len(neg_freq)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word amount\n",
    "|minimum_freq|total words|total unique* words|\n",
    "|-|-|-|\n",
    "|1|pos=7102, neg=13476|pos=2505, neg=8879|\n",
    "|2|pos=3388, neg=6630|pos=688, neg=3930|\n",
    "|3|pos=2426, neg=4959|pos=367, neg=2900|\n",
    "|5|pos=1658, neg=3506|pos=206, neg=2054|\n",
    "|10|pos=1018, neg=2233|pos=110, neg=1325|\n",
    "|50|pos=258, neg=604|pos=43, neg=389|\n",
    "|100|pos=113, neg=307|pos=25, neg=219|\n",
    "\n",
    "\\* Unique words - words that are not present in another group. Words exept the intersection of pos, neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #1\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.67 |\n",
    "| 2 | 0.69 |\n",
    "| 3 | 0.68 |\n",
    "| 5 | 0.70 |\n",
    "| 10 | 0.71 |\n",
    "| 50 | 0.73 |\n",
    "| 100 | 0.73 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0204440d6e8e43fcab9bd83bb0303cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ceae85f79f469f8a9197d9deefbfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7323\n"
     ]
    }
   ],
   "source": [
    "def eval_sentiment_1(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    tokens_unique = set(tokens)\n",
    "    sent_eval = 0\n",
    "    sent_eval += len(tokens_unique.intersection(pos_unique))\n",
    "    sent_eval -= len(tokens_unique.intersection(neg_unique))\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_1(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_1(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #2\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.62 |\n",
    "| 2 | 0.63 |\n",
    "| 3 | 0.61 |\n",
    "| 5 | 0.61 |\n",
    "| 10 | 0.60 |\n",
    "| 50 | 0.58 |\n",
    "| 100 | 0.56 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6820ab08164c8a86d1f597f10a6ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec36b77ede8f45e980b9ca3c55797ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5580\n"
     ]
    }
   ],
   "source": [
    "def eval_sentiment_2(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    sent_eval = 0\n",
    "    for tkn in tokens:\n",
    "        sent_eval += pos_normalized_freq[tkn] - neg_normalized_freq[tkn]\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_2(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_2(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method #3\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.68 |\n",
    "| 2 | 0.70 |\n",
    "| 3 | 0.70 |\n",
    "| 5 | 0.72 |\n",
    "| 10 | 0.74 |\n",
    "| 50 | 0.81 |\n",
    "| 100 | 0.83 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94ad332cddc424181845691f5a4e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be79490eaa384ccea62eada0d1c2f424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8347\n"
     ]
    }
   ],
   "source": [
    "def eval_sentiment_3(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    sent_eval = 0\n",
    "    for tkn in tokens:\n",
    "        sent_eval += pos_normalized_unique_freq[tkn] - neg_normalized_unique_freq[tkn]\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method #4\n",
    "| minimum_freq | accuracy |\n",
    "| - |  -   |\n",
    "| 1 | 0.77 |\n",
    "| 2 | 0.80 |\n",
    "| 3 | 0.80 |\n",
    "| 5 | 0.80 |\n",
    "| 10 | 0.77 |\n",
    "| 50 | 0.74 |\n",
    "| 100 | 0.74 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb26a2a156a488d979ea04856555e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbca9985d9d455dabd118977f4b80d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7408\n"
     ]
    }
   ],
   "source": [
    "def eval_sentiment_3(text) -> int:\n",
    "    \"\"\"Returns 1 if positive and -1 if negative\"\"\"\n",
    "    tokens = tokenizing_pipeline(text)\n",
    "    tokens_unique = set(tokens)\n",
    "    sent_eval = 0\n",
    "    pos_eval = len(tokens_unique.intersection(pos_unique))\n",
    "    neg_eval = len(tokens_unique.intersection(neg_unique))\n",
    "    if pos_eval == 0 and neg_eval == 0:\n",
    "        for tkn in tokens:\n",
    "            sent_eval += pos_normalized_freq[tkn] - neg_normalized_freq[tkn]\n",
    "    else:\n",
    "        sent_eval = pos_eval - neg_eval\n",
    "    return 1 if sent_eval > 0 else -1\n",
    "\n",
    "correct = incorrect = 0\n",
    "\n",
    "for txt in tqdm(pos_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == 1)\n",
    "    incorrect += 1 * (sentiment != 1)\n",
    "for txt in tqdm(neg_test):\n",
    "    sentiment = eval_sentiment_3(txt)\n",
    "    correct += 1 * (sentiment == -1)\n",
    "    incorrect += 1 * (sentiment != -1)\n",
    "\n",
    "print(f\"Accuracy: {correct / (correct + incorrect):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performances\n",
    "- accuracy=0.83 - method #3 - minimum_freq=100\n",
    "- accuracy=0.80 - method #4 - minimum_freq=3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
